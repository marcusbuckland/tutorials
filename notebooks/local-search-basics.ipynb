{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Search: Basics\n",
    "\n",
    "---\n",
    "\n",
    "**Local search** is a very commonly used heuristic for solving complex combinatorial optimisation problems. In this tutorial, we will introduce the basics of local search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Search Framework\n",
    "\n",
    "---\n",
    "\n",
    "The idea of local search is very simple. It starts with an **initial solution**. At each step, it explores the **neighborhood** of the current solution, and **move to a neighbouring solution**.\n",
    "\n",
    "<img src=\"img/ls.png\" width=500 />\n",
    "\n",
    "The framework of the local search is as follows.\n",
    "\n",
    "```Python\n",
    "def local_search_framework(problem):\n",
    "    sol = initialise(problem)\n",
    "    while not stop:\n",
    "        neighbour = select_from_neighbourhood(sol, problem)\n",
    "    \n",
    "        if accept(neighbour, sol, problem):\n",
    "            sol = neighbour\n",
    "            \n",
    "    return sol\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The local search has the following design issues:\n",
    "\n",
    "1. How to **initialise** the solution?\n",
    "2. How to **define the neighbourhood**?\n",
    "3. How to **select the neighbour** from the neighbourhood of the current solution?\n",
    "4. How to decide whether to **accept the selected neighbour** (move to the neighbour) or not?\n",
    "5. How to define the **stopping criteria**?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution Initialisation\n",
    "\n",
    "---\n",
    "\n",
    "The solution initialisation is problem specific. \n",
    "\n",
    "The most straightforward way is to **randomly generate** a solution. For example, for the traveling salesman problem where a solution is a permutation of the given set of nodes, we can randomly generate a solution by randomly shuffling the nodes. For the knapsack problem to select a subset of items subject to the capacity constraint, we can randomly pick the items until there is no item to be added without violating the capacity constraint.\n",
    "\n",
    "We could also use **constructive heuristics** to initialise the solution. Examples include the [nearest neighbour and insertion heuristics for traveling salesman problem](https://github.com/meiyi1986/tutorials/blob/master/notebooks/tsp-greedy.ipynb), and [efficiency first heuristic for knapsack problem](https://github.com/meiyi1986/tutorials/blob/master/notebooks/knapsack-greedy.ipynb). The constructive heuristics can give much better initial solutions than random generation. However, they might also be too greedy and restrict the search within a narrow region around it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neighbourhood Definition\n",
    "\n",
    "---\n",
    "\n",
    "The neighbourhood definition is also problem specific.\n",
    "\n",
    "In **continuous optimisation** where the decision variables $\\mathbf{x}$ are continuous numbers, the neighbourhood definition is straightforward and intuitive. We can say that the neighbourhood of a solution $\\mathbf{x}$ is the region (set of solutions) within a certain radius around it, i.e., \n",
    "\n",
    "$$\n",
    "\\mathcal{N}_{\\delta}(\\mathbf{x}) = \\{\\mathbf{x}' \\text{ with } ||\\mathbf{x}' - \\mathbf{x}|| < \\delta\\},\n",
    "$$\n",
    "\n",
    "where $\\delta$ is the radius parameter.\n",
    "\n",
    "However, in **combinatorial optimisation**, the neighbourhood structure is not straightforward due to the discontinuous, non-smooth and irregular structure of solutions, such as a permutation (routing and scheduling) or binary string (knapsack problem)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Move Operators\n",
    "\n",
    "In general, the neighbourhood of combinatorial optimisation problems is defined based on **move operators**, which is an operator to (slightly) modify the solution. Specifically, given a solution $x$, a move operator $\\mathtt{op}$ and its associated parameters $\\boldsymbol{\\theta}$, we can obtain a new solution\n",
    "\n",
    "$$\n",
    "x' = \\mathtt{op}(x, \\boldsymbol{\\theta}).\n",
    "$$\n",
    "\n",
    "Then, the **neighbourhood** based on the move operator $\\mathtt{op}$ is defined as the set of $x' = \\mathtt{op}(x, \\boldsymbol{\\theta})$ for all the possible $\\boldsymbol{\\theta}$ values, i.e.,\n",
    "\n",
    "$$\n",
    "\\mathcal{N}_{\\mathtt{op}}(x) = \\bigcup_{\\boldsymbol{\\theta} \\in \\boldsymbol{\\Theta}} \\mathtt{op}(x, \\boldsymbol{\\theta}),\n",
    "$$\n",
    "\n",
    "where $\\boldsymbol{\\Theta}$ is the set of all the possible $\\boldsymbol{\\theta}$ values.\n",
    "\n",
    "Below are some examples of commonly used move operators for combinatorial optimisation problems:\n",
    "\n",
    "- **2-opt** for traveling salesman problem: Given a tour, which is a permutation of nodes $[x_1, \\dots, x_n, x_1]$, the 2-opt operator selects a sub-tour $[x_i, \\dots, x_j]$, and reverse it. The resultant solution is $[x_1, \\dots, x_{i-1}, x_j, x_{j-1}, \\dots, x_{i}, x_{j+1}, \\dots, x_1]$. The associated parameters are $i$ and $j$.\n",
    "- **Bit flip** for binary optimisation such as knapsack problem: Given a bit string, the bit flip operator flips the bit (1 to 0, 0 to 1) of a position $i$. The associated parameter is $i$.\n",
    "\n",
    "> **NOTE**: **The design of move operators is the key to the success of local search.** However, designing effective move operators heavily relies on domain knowledge and expertise.\n",
    "\n",
    "> **NOTE**: The neighbourhood of combinatorial optimisation generally consists of a **finite number** of neighbours, although the number can be large. In contrast, the continuous optimisation typically has an infinite neighbours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Optimum\n",
    "\n",
    "Based on the defined neighbourhood, the concept of **local optimum** is defined as follows.\n",
    "\n",
    "> **DEFINITION**: A solution $x^*$ is a local optimum in the neighbourhood $\\mathcal{N}(\\cdot)$, if there is no neighbour $x' \\in \\mathcal{N}(x^*)$ that is better than $x^*$.\n",
    "\n",
    "Note that whether a solution $x$ is a local optimum or not depends on the neighbourhood $\\mathcal{N}(\\cdot)$, which determines the set of neighbours. A solution might be a local optimum in one neighbourhood $\\mathcal{N}_1(\\cdot)$, but not a local optimum in another neighbourhood $\\mathcal{N}_2(\\cdot)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neighbour Selection\n",
    "\n",
    "---\n",
    "\n",
    "Below are some commonly used neighbour selection schemes.\n",
    "\n",
    "- Randomly sample a neighbour from the neighbourhood.\n",
    "- Select the first neighbour that is better than the current solution (**first improvement**).\n",
    "- Select the best neighbour in the neighbourhood (**best improvement**).\n",
    "\n",
    "Different local search algorithms might use different neighbour selection schemes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neighbour Acceptance\n",
    "\n",
    "---\n",
    "\n",
    "The common neighbour acceptance criteria are as follows.\n",
    "\n",
    "- Accept the neighbour if it is better than the current solution.\n",
    "- Always accept the neighbour.\n",
    "- Accept based on probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopping Criteria\n",
    "\n",
    "---\n",
    "\n",
    "There are several possible stopping criteria that can be used.\n",
    "\n",
    "- When no improvement is found in the current neighbourhood (the search is stuck in a local optimum).\n",
    "- When a certain number of iterations is reached."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Local Search Algorithms\n",
    "\n",
    "---\n",
    "\n",
    "Now, we introduce some common local search algorithms and how they fit into the local search framework and address the design issues in different ways. In general, the solution initialisation and neighbourhood definition are problem specific, and shared by different local search algorithms. Here, we focus on **Neighbour Selection**, **Neighbour Acceptance** and **Stopping Criteria**.\n",
    "\n",
    "> **NOTE**: Without loss of generality, we assume minimisation problem $\\min f(x)$.\n",
    "\n",
    "### Hill Climbing\n",
    "\n",
    "[Hill Climbing](https://en.wikipedia.org/wiki/Hill_climbing) is the most basic local search algorithm. It keeps moving to the best neighbour until reaching a local optimum.\n",
    "\n",
    "#### Neighbour Selection\n",
    "\n",
    "At each step, the **best neighbour** in the neighbourhood of the current solution is selected, i.e., \n",
    "\n",
    "```Python\n",
    "def select_from_neighbourhood(sol, problem):\n",
    "    return argmin([f(nb) for nb in neighbourhood(sol)])\n",
    "```\n",
    "\n",
    "#### Neighbour Acceptance\n",
    "\n",
    "The neighbour is **accepted if it is better** than the current solution, i.e., \n",
    "\n",
    "```Python\n",
    "def accept(nb, sol, problem):\n",
    "    return f(nb) < f(sol)\n",
    "```\n",
    "\n",
    "#### Stopping Criteria\n",
    "\n",
    "The search is stopped after reaching a local optimum. That is, `obj(nb) >= obj(sol)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulated Annealing\n",
    "\n",
    "[Simulated Annealing](https://en.wikipedia.org/wiki/Simulated_annealing) is inspired by the metal annealing process. It might jump out of local optima with a probability controlled by a temperature parameter.\n",
    "\n",
    "#### Neighbour Selection\n",
    "\n",
    "At each step, a neighbour is **randomly** sampled from the neighbourhood of the current solution, i.e., \n",
    "\n",
    "```Python\n",
    "def select_from_neighbourhood(sol, problem):\n",
    "    randomly sample a neighbour from neighbourhood(sol)\n",
    "```\n",
    "\n",
    "#### Neighbour Acceptance\n",
    "\n",
    "A neighbour is **always accepted if it is better** than the current solution. **Otherwise it is accepted with probability** $e^{\\frac{f(x)-f(x')}{T}}$, where $T \\geq 0$ is the temperature.\n",
    "\n",
    "```Python\n",
    "def accept(nb, sol, problem):\n",
    "    if f(nb) < f(sol):\n",
    "        return True\n",
    "    elif random.rand() < exp((f(sol) - f(nb)) / T):\n",
    "        return True\n",
    "    return False\n",
    "```\n",
    "\n",
    "#### Stopping Criteria\n",
    "\n",
    "The search is stopped after a certain number of iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tabu Search\n",
    "\n",
    "[Tabu Search](https://en.wikipedia.org/wiki/Tabu_search) employs a tabu list to store the search memory. The search is guided by the tabu list not to go back to previously visited solutions, so as to escape from local optima.\n",
    "\n",
    "#### Neighbour Selection\n",
    "\n",
    "At each step, the **best non-tabu** neighbour in the neighbourhood of the current solution is selected, i.e.,\n",
    "\n",
    "```Python\n",
    "def select_from_neighbourhood(sol, problem):\n",
    "    argmin([f(nb) for nb in neighbourhood(sol) if nb not in tabu_list])\n",
    "```\n",
    "\n",
    "#### Neighbour Acceptance\n",
    "\n",
    "A neighbour is **always accepted** no matter whether it is better than the current solution or not, i.e.,\n",
    "\n",
    "```Python\n",
    "def accept(nb, sol, problem):\n",
    "    return True\n",
    "```\n",
    "\n",
    "#### Stopping Criteria\n",
    "\n",
    "The search is stopped after a certain number of iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guided Local Search\n",
    "\n",
    "[Guided Local Search](https://en.wikipedia.org/wiki/Guided_Local_Search) employs a utility function to penalise each feature of solutions, and accept neighbours based on the **augmented** objective function by the utility instead of the original objective function. By penalisation, the local optimum's augmented objective function becomes worse and worse, and the search can jump to other neighbours.\n",
    "\n",
    "#### Neighbour Selection\n",
    "\n",
    "At each step, the neighbour in the neighbourhood of the current solution with the best augmented function is selected, i.e.,\n",
    "\n",
    "```Python\n",
    "def select_from_neighbourhood(sol, problem):\n",
    "    argmin([g(nb) for nb in neighbourhood(sol)])\n",
    "```\n",
    "\n",
    "where `g(nb)` is the augmented objective function with the utility penalisation.\n",
    "\n",
    "#### Neighbour Acceptance\n",
    "\n",
    "A neighbour is **accepted if it has better augmented objective function** than the current solution, i.e.,\n",
    "\n",
    "```Python\n",
    "def accept(nb, sol, problem):\n",
    "    return g(nb) < g(sol)\n",
    "```\n",
    "\n",
    "#### Stopping Criteria\n",
    "\n",
    "The search is stopped after a certain number of iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jump Out of Local Optima\n",
    "\n",
    "---\n",
    "\n",
    "Local search is easy to be trapped into local optima, especially for complex multi-modal problems with many local optima. Therefore, it is important to design mechanisms for local search to **jump out of local optima**.\n",
    "\n",
    "Below are some commonly used strategies to jump out of local optima. \n",
    "\n",
    "### Accept Worse Neighbours\n",
    "\n",
    "We can accept worse neighbours to help the search move out of the current valley and find another (better) local optimum. Example algorithms include simulated annealing (accept with some probability) and tabu search (accept non-tabu worse neighbours).\n",
    "\n",
    "### Change Neighbourhood\n",
    "\n",
    "Noted that local optimum depends on the neighbourhood. A local optimum in one neighbourhood might become a non-local optimum in another neighbourhood. Thus, we can change the neighbourhood (usually to a larger neighbourhood) to make the current local optimum a non-local optimum, so the search can jump out of the current local optimum. An example algorithm is the [variable neighbourhood search](https://en.wikipedia.org/wiki/Variable_neighborhood_search).\n",
    "\n",
    "### Change Evaluation\n",
    "\n",
    "We can also change the evaluation scheme to consider additional aspects to the original objective function. For example, we can penalise solutions whose building blocks have been explored too much, so that we can explore new solutions that have not been seen before. An example algorithm is the guided local search.\n",
    "\n",
    "### Restart\n",
    "\n",
    "After the search gets stuck, we can restart from a new initial solution and do the local search again. If the new initial solution is substantially different from the current local optimum (and the solutions examined before), then we are likely to find a new (possibly better) local optimum.\n",
    "\n",
    "The new initial solution could be obtained randomly or perturbing the current local optimum. This is the idea of the [iterated local search](https://en.wikipedia.org/wiki/Iterated_local_search)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "- More tutorials can be found [here](https://github.com/meiyi1986/tutorials).\n",
    "- [Yi Mei's homepage](https://meiyi1986.github.io/)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMeJhjW6Pj0jBqaT2hJqUdO",
   "name": "ortools-jss.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
